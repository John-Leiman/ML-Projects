{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Go through the iris_ml.ipnyb summary for most of the background.\n",
        "\n",
        "Going off of that file, using \"Keras\" (which is based on either theano or, in my case, tensorflow) for deep-learning simplifies the programmer's job tons."
      ],
      "metadata": {
        "id": "2fxE_ldHmc6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup\n",
        "---"
      ],
      "metadata": {
        "id": "ltGHb5EDm4Sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm having so much trouble with missing and incompatible Keras imports. Thankfully \"scikeras\" exists; it's some sort of library that makes working between Keras and scikit smoother."
      ],
      "metadata": {
        "id": "eVGdwYzynVlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikeras"
      ],
      "metadata": {
        "id": "o_idsNJpODt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL9QNX4H7jXE"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are format issues I was struggling with to get the scikit iris dataframe to work with keras, so I just downloaded the csv file [online](https://archive.ics.uci.edu/dataset/53/iris), and called the file from the directory. I use LabelEncoder() and to_categorical() to turn the Iris class data into a dummy_variable.\n",
        "\n",
        "split the data like usual."
      ],
      "metadata": {
        "id": "0YAL3JMwoI8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "dummy_Y = to_categorical(encoded_Y)\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, dummy_Y, test_size=0.20, random_state=1)"
      ],
      "metadata": {
        "id": "R69sML0974s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep-Learning\n",
        "---"
      ],
      "metadata": {
        "id": "Bt8Q1ta6m-WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the meat 'n potatoes here:\n",
        "\n",
        "As I understand it, these DL models (I don't know that this is a common acronym), stand on neural networks as background data structure, inspired by the human brain and Cognitive Science. They are more sophisticated than traditional ML models, and I think you can say \"cover their bases\". I'll learn more about these when I get to take CSE 151B, but the important part is I know how to train them."
      ],
      "metadata": {
        "id": "wGuQE0lpo72k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "fwf7xG2h_w7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proceeding to train the deep-learning model:\n",
        "\n",
        "Interestingly, an alternative to calling fit(), is to just throw the whole dataset into cv_score. But cv_score doesn't return a whole \"classification report\" like .fit() does and lacks metrics like precision, recall f1-score etc."
      ],
      "metadata": {
        "id": "SbLVzB3mpr-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = KerasClassifier(model=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "cv_score = cross_val_score(estimator, X, dummy_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (cv_score.mean()*100, cv_score.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykF2sycr_-_G",
        "outputId": "b488f2f2-eb0b-4745-80d8-f1e5b02763f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: 97.33% (3.27%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator.fit(X_train, Y_train)\n",
        "predictions = estimator.predict(X_validation)\n",
        "print(classification_report(Y_validation, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx4JMlWjsHR0",
        "outputId": "3601627d-f9d1-4f55-cb23-15a9e288ea88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.86      1.00      0.92         6\n",
            "\n",
            "   micro avg       0.97      0.97      0.97        30\n",
            "   macro avg       0.95      0.97      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            " samples avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly not only did the DL training take much longer than traditional ML training, the results were not actually better. This is because 1. the Iris dataset is not actually very nuanced, so there are simply outliers that it would not necessarily be reasonable to expect the models to catch, & 2. in general the more sophisticated the heuristics (fancy word for guessing pattern) are, the more data would be required to generalize and find patterns.\n",
        "In other words, point 2 would mean that the DL model is \"overthinking it\" and amazingly seeing connections that aren't there (which humans are very prone to do). This is called overfitting.\n",
        "\n",
        "End of notebook"
      ],
      "metadata": {
        "id": "GtUL4Hyoy9bQ"
      }
    }
  ]
}